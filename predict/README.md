# RAG LLM Slim Endpoint

This library deploys an endpoint on Cnvrg that is used to ask questions and generate relevant answers, based on the documents present in the connected cnvrg dataset. This endpoint connects with a Large Language Model, based on the information provided in the environment variables. Currently we support:

- txt-to-txt generation models from huggingface
- LLMs hosted on Cnvrg
- OpenAI InstructGPT models.

## Flow

1. The User asks RAG endpoint a question
2. The RAG Endpoint retrieves top n relevant documents from cnvrg dataset.
3. The RAG Endpoint re-ranks the documents to have only top k results.
4. The RAG Endpoint shares the top k results along with the user-defined prompt to the LLM.
5. The response generated by the LLM is returned to the user.

## Setup 

The user  must add a few environment variables in order to deploy and use the RAG endpoint. [Setting up Environment Variables in Cnvrg.](https://app.cnvrg.io/docs/core_concepts/projects.html#environment)

### Dataset setup
The user must add the documents to the cnvrg dataset in a json format

Sample of the dataset document for reference:

```
[
    {
        "content": "I'm experiencing nasal congestion and pain in my eye. What could be wrong? Based on your symptoms, it seems like you are suffering from conjunctivitis due to bacteria."
    },
    {
        "content": "I'm experiencing high fever, severe headache, and joint pain. Okay, there's a blood test called hematologic test which we need to do to confirm whether you have Dengue fever or not."
    }
]
``` 
Note: Adjusting the document's format is possible to meet the model's requirements. However, it's essential to modify how the fucntion `updator` within the `predict.py` script accesses the document.

### LLM setup variables
`PROVIDER` = The name of the LLM service provider. Acceptable values are: `huggingface` , `openai` and `cnvrg`

`MODEL_NAME` = in case the LLM is provided by huggingface, provide repo id for example: `google/flan-t5-xl` or if the model is provided by openAI provide model name for example: `gpt-3.5-turbo`

`PROMPT` = Enter the prompt you want to share with the LLM. Your prompt will include the question you ask the RAG endpoint and the documents. Query and documents will be automatically inserted in the placeholders. An example prompt is given below.

    
    Below is an instruction that describes a task paired with an input, which provides further context. Write a response that appropriately completes the request.

    ### Instruction:
    You are a doctor. Synthesize a comprehensive answer from the following Input and the question: {query}

    ### Input:
    paragraphs: {documents}

    ### Response:

Note: The prompt can be modified to fit the model needs

`API_KEY` = Your API key provided by the service provider. 

`URL` = The URL of your model in case it is deployed as endpoint on cnvrg..

**Note:** For users whose LLM model is hosted on Cnvrg, follow the steps below to get the API_KEY and URL. 

Once deployed, go to the endpoint page and scroll down.

    curl -X POST \
        https://your-endpoint.cloud.cnvrg.io/api/v1/endpoints/XYZ \
    -H 'Cnvrg-Api-Key: XXXX' \
    -H 'Content-Type: application/json' \
    -d '{"input_params": "your_input_params"}'
    
`https://your-endpoint.cloud.cnvrg.io/api/v1/endpoints/XYZ` is your `URL`
`XXXX` is your `API_KEY`

### RAG setup variables

`RETRIEVER_N` = The  number of top documents you want the retriever to retrieve from the ElasticSearch index. For example 20.

`RANKER_N` = The  number of top documents you want to send to the LLM after re-ranking. For example 5.